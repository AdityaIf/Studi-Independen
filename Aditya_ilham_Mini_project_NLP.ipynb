{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aditya ilham Mini project NLP",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaIf/Studi-Independen/blob/main/Aditya_ilham_Mini_project_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PiIokfFzAXZQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKOQ0lt85NGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1303b1cb-584a-4eb9-9886-70f60a2111bc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_7U5HYQ9hFa"
      },
      "source": [
        "# Analisis Sentimen dengan TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-QOW1EUB3-u"
      },
      "source": [
        "Text Classification adalah proses klasifikasi data dalam bentuk teks seperti tweet, review, artikel, dan blog, ke dalam kategori yang telah ditentukan. Analisis Sentimen adalah special case dari Text Classification, yaitu di mana pendapat atau sentimen pengguna tentang produk apa pun dapat diprediksi dari data tekstual.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoySzi2mRHAa"
      },
      "source": [
        "# 1. Problem Scoping"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dalam tutorial ini, Kita akan belajar mengembangkan model Analisis Sentimen yang akan menggunakan pendekatan pada pembuatan fitur TF-IDF dan akan mampu memprediksi sentimen pengguna (yaitu pandangan atau pendapat yang dimiliki atau diungkapkan) tentang ANALISIS SENTIMEN HASHTAG ‚ÄúDIRUMAHAJA‚Äù SAAT PANDEMI\n",
        "COVID-19 DI INDONESIA MENGGUNAKAN NLP. Kita akan menggunakan salah satu library Python untuk machine learning yaitu Scikit-Learn, Untuk mengimplementasikan penggunaan dari fitur TF-IDF dan melatih prediction model."
      ],
      "metadata": {
        "id": "2k5mKeBSRARE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqtIn6beRU9A"
      },
      "source": [
        "# 2. Data Akuisisi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SViBdI7F9SaZ"
      },
      "source": [
        "## Melakukan Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAcgcQJx9VsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d29fb4-b47d-4d5f-b285-c8751c673c36"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import re  \n",
        "import nltk \n",
        "nltk.download('stopwords')  \n",
        "from nltk.corpus import stopwords\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc1_REBT9Z9j"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fIcg5HBCGni"
      },
      "source": [
        "Gunakan metode read_csv dari library Pandas untuk memuat dataset ke dalam kerangka data \"tweets\" (*). Anda dapat menggunakan URL online atau mengunduh file dan menggunakan jalur lokal file CSV di mesin Anda.\n",
        "\n",
        "Langkah selanjutnya, muat dataset yang akan digunakan untuk melatih model. Seperti yang kita bicarakan sebelumnya, Kita akan membangun model analisis sentimen untuk memprediksi sentimen publik tentang 6 maskapai besar yang beroperasi di Amerika Serikat. Dataset tersedia secara bebas di tautan Github ini. Sentimen yang digunakan yaitu hanya positive dan negative.\n",
        "\n",
        "Gunakan metode read_csv dari library Pandas untuk memuat dataset ke dalam dataframe ‚Äútweets‚Äù. Kita dapat menggunakan URL online atau mengunduh file dan menggunakan lokal path file CSV di drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj6jqBn09e-o"
      },
      "source": [
        "data = pd.read_csv(\"https://raw.githubusercontent.com/rasyidev/well-known-datasets/main/Tweets.csv\")\n",
        "\n",
        "t_1 = data[data['airline_sentiment']=='positive']#.sample(100,replace=True)\n",
        "t_2 = data[data['airline_sentiment']=='negative']#.sample(400,replace=True)\n",
        "tweets = pd.concat([t_1, t_2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvXKzblMCMWm"
      },
      "source": [
        "(*) DataFrame adalah struktur data dua dimensi, sehingga data diselaraskan dalam bentuk seperti tabel, yaitu dalam baris dan kolom. Ini umumnya merupakan objek dari Pandas yang paling umum digunakan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5KzFK3bCTLB"
      },
      "source": [
        "Untuk melihat bagaimana dataset terlihat, gunakan metode head() dari dataframe Pandas, yang akan menampilkan 5 baris pertama atau metode tail() untuk menampilkan 5 baris terbawah dari dataset seperti yang ditunjukkan di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZCeMEY69y5q",
        "outputId": "cee7d703-e240-4827-c62f-250851db52db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "# Cek 5 Dataset teratas\n",
        "tweets.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
              "1   570301130888122368          positive                        0.3486   \n",
              "6   570300616901320704          positive                        0.6745   \n",
              "8   570299953286942721          positive                        0.6559   \n",
              "9   570295459631263746          positive                        1.0000   \n",
              "11  570289724453216256          positive                        1.0000   \n",
              "\n",
              "   negativereason  negativereason_confidence         airline  \\\n",
              "1             NaN                        0.0  Virgin America   \n",
              "6             NaN                        0.0  Virgin America   \n",
              "8             NaN                        NaN  Virgin America   \n",
              "9             NaN                        NaN  Virgin America   \n",
              "11            NaN                        NaN  Virgin America   \n",
              "\n",
              "   airline_sentiment_gold          name negativereason_gold  retweet_count  \\\n",
              "1                     NaN      jnardino                 NaN              0   \n",
              "6                     NaN    cjmcginnis                 NaN              0   \n",
              "8                     NaN      dhepburn                 NaN              0   \n",
              "9                     NaN    YupitsTate                 NaN              0   \n",
              "11                    NaN  HyperCamiLax                 NaN              0   \n",
              "\n",
              "                                                 text tweet_coord  \\\n",
              "1   @VirginAmerica plus you've added commercials t...         NaN   \n",
              "6   @VirginAmerica yes, nearly every time I fly VX...         NaN   \n",
              "8     @virginamerica Well, I didn't‚Ä¶but NOW I DO! :-D         NaN   \n",
              "9   @VirginAmerica it was amazing, and arrived an ...         NaN   \n",
              "11  @VirginAmerica I &lt;3 pretty graphics. so muc...         NaN   \n",
              "\n",
              "                tweet_created    tweet_location               user_timezone  \n",
              "1   2015-02-24 11:15:59 -0800               NaN  Pacific Time (US & Canada)  \n",
              "6   2015-02-24 11:13:57 -0800  San Francisco CA  Pacific Time (US & Canada)  \n",
              "8   2015-02-24 11:11:19 -0800         San Diego  Pacific Time (US & Canada)  \n",
              "9   2015-02-24 10:53:27 -0800       Los Angeles  Eastern Time (US & Canada)  \n",
              "11  2015-02-24 10:30:40 -0800               NYC            America/New_York  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e93e157-0243-4c91-9804-77ba8569f091\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>570300616901320704</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.6745</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cjmcginnis</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:13:57 -0800</td>\n",
              "      <td>San Francisco CA</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>570299953286942721</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.6559</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>dhepburn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@virginamerica Well, I didn't‚Ä¶but NOW I DO! :-D</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:11:19 -0800</td>\n",
              "      <td>San Diego</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>570295459631263746</td>\n",
              "      <td>positive</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>YupitsTate</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 10:53:27 -0800</td>\n",
              "      <td>Los Angeles</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>570289724453216256</td>\n",
              "      <td>positive</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HyperCamiLax</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I &amp;lt;3 pretty graphics. so muc...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 10:30:40 -0800</td>\n",
              "      <td>NYC</td>\n",
              "      <td>America/New_York</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e93e157-0243-4c91-9804-77ba8569f091')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6e93e157-0243-4c91-9804-77ba8569f091 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6e93e157-0243-4c91-9804-77ba8569f091');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he3iRAI1VgcQ",
        "outputId": "e3fabce1-ec8b-49d1-95da-6efa581a2e36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "# Cek 5 Dataset terbawah\n",
        "tweets.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
              "14631  569588464896876545          negative                        1.0000   \n",
              "14633  569587705937600512          negative                        1.0000   \n",
              "14634  569587691626622976          negative                        0.6684   \n",
              "14636  569587371693355008          negative                        1.0000   \n",
              "14638  569587188687634433          negative                        1.0000   \n",
              "\n",
              "               negativereason  negativereason_confidence   airline  \\\n",
              "14631              Bad Flight                     1.0000  American   \n",
              "14633        Cancelled Flight                     1.0000  American   \n",
              "14634             Late Flight                     0.6684  American   \n",
              "14636  Customer Service Issue                     1.0000  American   \n",
              "14638  Customer Service Issue                     0.6659  American   \n",
              "\n",
              "      airline_sentiment_gold             name negativereason_gold  \\\n",
              "14631                    NaN         MDDavis7                 NaN   \n",
              "14633                    NaN  RussellsWriting                 NaN   \n",
              "14634                    NaN    GolfWithWoody                 NaN   \n",
              "14636                    NaN         itsropes                 NaN   \n",
              "14638                    NaN       SraJackson                 NaN   \n",
              "\n",
              "       retweet_count                                               text  \\\n",
              "14631              0  @AmericanAir thx for nothing on getting us out...   \n",
              "14633              0  @AmericanAir my flight was Cancelled Flightled...   \n",
              "14634              0         @AmericanAir right on cue with the delaysüëå   \n",
              "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
              "14638              0  @AmericanAir you have my money, you change my ...   \n",
              "\n",
              "      tweet_coord              tweet_created tweet_location  \\\n",
              "14631         NaN  2015-02-22 12:04:07 -0800             US   \n",
              "14633         NaN  2015-02-22 12:01:06 -0800    Los Angeles   \n",
              "14634         NaN  2015-02-22 12:01:02 -0800            NaN   \n",
              "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
              "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
              "\n",
              "                    user_timezone  \n",
              "14631  Eastern Time (US & Canada)  \n",
              "14633                     Arizona  \n",
              "14634                       Quito  \n",
              "14636                         NaN  \n",
              "14638  Eastern Time (US & Canada)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a833669-8c63-4d5b-b8be-b0a8baa5e24d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14631</th>\n",
              "      <td>569588464896876545</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MDDavis7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir thx for nothing on getting us out...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 12:04:07 -0800</td>\n",
              "      <td>US</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14633</th>\n",
              "      <td>569587705937600512</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Cancelled Flight</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RussellsWriting</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir my flight was Cancelled Flightled...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 12:01:06 -0800</td>\n",
              "      <td>Los Angeles</td>\n",
              "      <td>Arizona</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14634</th>\n",
              "      <td>569587691626622976</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.6684</td>\n",
              "      <td>Late Flight</td>\n",
              "      <td>0.6684</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GolfWithWoody</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir right on cue with the delaysüëå</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 12:01:02 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Quito</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14636</th>\n",
              "      <td>569587371693355008</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Customer Service Issue</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>itsropes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 11:59:46 -0800</td>\n",
              "      <td>Texas</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14638</th>\n",
              "      <td>569587188687634433</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Customer Service Issue</td>\n",
              "      <td>0.6659</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SraJackson</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir you have my money, you change my ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 11:59:02 -0800</td>\n",
              "      <td>New Jersey</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a833669-8c63-4d5b-b8be-b0a8baa5e24d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8a833669-8c63-4d5b-b8be-b0a8baa5e24d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8a833669-8c63-4d5b-b8be-b0a8baa5e24d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm8Hyul6CbSp"
      },
      "source": [
        "Begitu pula untuk menampilkan atau menemukan jumlah baris dan kolom dalam dataset, Anda dapat menggunakan atribut  columns dan shape seperti yang ditunjukkan di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYoWEuXX-F82"
      },
      "source": [
        "# Cek attribut dataset\n",
        "display(tweets.columns)\n",
        "\n",
        "# Cek jumlah baris dan kolom dataset\n",
        "display(tweets.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-rInDxJCgnF"
      },
      "source": [
        "Dalam output, Kita melihat ada (11541, 15) yang berarti bahwa dataset kami terdiri dari 11541 baris dan 15 kolom. Namun, di antara kolom, kita hanya tertarik pada kolom ‚Äúairline_sentiment‚Äù yang terdiri dari kategori sentimen sebenarnya, dan kolom ‚Äútext‚Äù yang berisi teks aktual dari tweet tersebut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbHUTw609ziv"
      },
      "source": [
        "## 3. Explorasi Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAFFBjhsCmLU"
      },
      "source": [
        "Jadi, sebelum kita membangun model yang sebenarnya. Mari kita lakukan beberapa eksplorasi analisis data pada model. Untuk melihat jumlah ulasan positif dan negatif dalam bentuk plot bar, jalankan skrip dibawah di mana library python dari Seaborn digunakan untuk menggambar metode countplot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URRkQxRK-S6y"
      },
      "source": [
        "# Cek jumlah review positive dan negative\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.countplot(x='airline_sentiment', data=tweets)\n",
        "plt.title('Distribusi class sentiment Airline', fontsize=16)\n",
        "plt.ylabel('Class Counts', fontsize=16)\n",
        "plt.xlabel('Class Label', fontsize=16)\n",
        "plt.xticks(rotation='vertical');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuG12_t3Cqm-"
      },
      "source": [
        "Dari output, Kita dapat melihat bahwa jumlah ulasan negatif jauh lebih tinggi daripada jumlah ulasan positif."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtYqTl6rCsFe"
      },
      "source": [
        "Demikian pula untuk melihat maskapai mana yang mendapat ulasan tertinggi, jalankan skrip berikut."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDZXKeN2-Tb4"
      },
      "source": [
        "# Cek Airline yang mendapat review paling tinggi\n",
        "sns.countplot(x='airline', data=tweets);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPMg6tQgCvOY"
      },
      "source": [
        "Dari output, Kita dapat melihat bahwa Maskapai Penerbangan ‚ÄúUnited‚Äù mendapat jumlah ulasan tertinggi sedangkan ‚ÄúVirgin America‚Äù mendapat jumlah ulasan terendah."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXKUXgIvC0Dc"
      },
      "source": [
        "Akhirnya, mari kita lihat jumlah review dari setiap jenis yang diterima setiap maskapai. Untuk melakukannya, kita dapat kembali menggunakan metode countplot dari library seaborn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7qNS5b_-gFG"
      },
      "source": [
        "# Cek jumlah review dari masing-masing Airline yang diterima\n",
        "sns.countplot(x='airline', hue=\"airline_sentiment\", data=tweets);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcSzZWvMC37G"
      },
      "source": [
        "Kita dapat melihat bahwa untuk hampir semua maskapai, jumlah ulasan negatif lebih besar daripada ulasan positif dan netral.\n",
        "\n",
        "Cukup untuk bagian analisis data eksplorasi, mari kita pindah ke bagian pemrosesan data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD0ZzsMLAUZd"
      },
      "source": [
        "## Pemrosesan Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWw0zLkjAjLU"
      },
      "source": [
        "Pertama, mari kita membagi dataset menjadi feature dan label set. Di feature set, Kita hanya akan menggunakan text tweet sebagai feature. Label yang sesuai akan menjadi sentimen dari tweet tersebut. Kolom text adalah kolom ke 10 (indeks kolom dimulai dari 0 di panda) dalam dataset dan berisi teks dari tweet tersebut. Demikian pula \"airline_sentiment\" adalah kolom pertama yang berisi sentimen. Gunakan metode \"iloc\" dari dataframe panda untuk membuat feature set X dan label set y, seperti yang ditunjukkan di bawah ini."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy2RNLgs-1dB"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X = tweets.iloc[:, 10].values  #kolom text\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit([\"positive\", \"negative\"])\n",
        "print(list(le.classes_))\n",
        "y = le.transform(tweets.iloc[:, 1].values) #kolom airline_sentiment\n",
        "\n",
        "#y = tweets.iloc[:, 1].values\n",
        "print(X.shape)\n",
        "print(X[0])\n",
        "print(y.shape)\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67i00fArA8k5"
      },
      "source": [
        "Dataset tersebut berisi banyak karakter khusus dan ruang kosong. Kita harus menghapusnya agar memiliki dataset yang bersih. Berikut script untuk melakukan itu:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB09_ADdAoIZ"
      },
      "source": [
        "# Membuat empty List\n",
        "processed_tweets = []\n",
        "\n",
        "for tweet in range(0, len(X)):  \n",
        "    # Hapus semua special characters\n",
        "    processed_tweet = re.sub(r'\\W', ' ', str(X[tweet]))\n",
        "\n",
        "    # Hapus semua single characters\n",
        "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
        "\n",
        "    # Hapus single characters dari awal\n",
        "    processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet) \n",
        "\n",
        "    # Substitusi multiple spaces dengan single space\n",
        "    processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
        "\n",
        "    # Hapus prefixed 'b'\n",
        "    processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
        "\n",
        "    # Ubah menjadi Lowercase\n",
        "    processed_tweet = processed_tweet.lower()\n",
        "\n",
        "    # Masukkan ke list kosong yang telah dibuat sebelumnya\n",
        "    processed_tweets.append(processed_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQt8zOdhBnCG"
      },
      "source": [
        "\n",
        "Mari kita lihat apa yang terjadi dalam skrip di atas. Pada dasarnya terdiri dari berbagai jenis ekspresi reguler untuk melakukan preprocessing teks. Ekspresi reguler `re.sub(r'\\W', ' ', str(X[tweet]))` berfungsi untuk menghapus semua karakter khusus dari tweet.\n",
        "\n",
        "Ketika menghapus karakter khusus, Kita hanya memiliki satu karakter yang tidak memiliki arti. Misalnya, ketika menghapus karakter khusus dari kata \"Julia's\", Kita dibiarkan \"Julia\" dan \"s\". Di sini \"s\" tidak memiliki arti. Ekspresi reguler `re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)` menghapus semua karakter tunggal kecuali yang ada di awal. Untuk menghapus karakter tunggal dari awal kalimat, digunakan regex `re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet)`.\n",
        "\n",
        "Selanjutnya yaitu hasil dari menghapus karakter khusus dan spasi tunggal, terdapat beberapa spasi muncul dalam teks. Untuk menghapus beberapa spasi ini dan menggantinya dengan spasi tunggal, gunakan regex `re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)`.\n",
        "\n",
        "Dalam beberapa kasus, dataset memiliki format byte. Dalam kasus seperti itu, karakter \"b\" ditambahkan di awal string. Hapus \"b\" yang berada didepan dengan menggunakan regex  `re.sub(r'^b\\s+', '', processed_tweet)`. Langkah terakhir yaitu ubah teks menjadi huruf kecil untuk menjaga keseragaman."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCpteX5ll1VL"
      },
      "source": [
        "# Cek sebelum cleaning data\n",
        "print(str(X[:5]))\n",
        "print()\n",
        "\n",
        "# Cek setelah cleaning data\n",
        "processed_tweets[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzIas1ktDJ5d"
      },
      "source": [
        "## Skema TF-IDF untuk Pembuatan Feature Numeric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O4t2IL4DLMi"
      },
      "source": [
        "Pendekatan statistik seperti machine learning dan deep learning dapat bekerja dengan baik menggunakan data numerik. Namun, natural language terdiri dari kata-kata dan kalimat. Karena itu, sebelum kita dapat membangun model analisis sentimen, Kita perlu mengubah teks menjadi angka. Beberapa pendekatan telah dikembangkan untuk mengubah teks menjadi angka. Bag of Words, N-gram, dan model Word2Vec adalah beberapa di antaranya.\n",
        "\n",
        "Pada pembahasan ini, Kita akan menggunakan pendekatan Bag of Words dengan skema TF-IDF, untuk mengkonversi teks menjadi angka. Library Python dari Sklearn dilengkapi dengan fungsi bawaan untuk mengimplementasikan pendekatan TF-IDF yang akan kita gunakan nanti. Di sini kita akan mempelajari wawasan singkat tentang pendekatan TF-IDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIykoAx7DOWL"
      },
      "source": [
        "### Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6kRajgnDQUv"
      },
      "source": [
        "Dalam pendekatan bag of words atau sekumpulan kata-kata, dimana semua kosakata dari kata-kata yang unik dalam semua dokumen terbentuk. Kosakata ini berfungsi sebagai feature vektor. Misalkan Anda memiliki tiga dokumen dalam corpus S1, S2, dan S3:\n",
        "* S1 = ‚ÄúIt is cold outside‚Äù\n",
        "* S2= ‚ÄúThe weather is cold‚Äù\n",
        "* S3 = ‚ÄúI am outside‚Äù\n",
        "\n",
        "Kosakata yang dibentuk menggunakan tiga kalimat di atas adalah:\n",
        "[it, is, cold, outside, the, weather, I,  am]\n",
        "\n",
        "Kosakata kata-kata ini akan digunakan untuk membuat feature vektor dari kalimat. Kita lihat bagaimana hal itu dilakukan feature vektor pada corpus S1:\n",
        "\n",
        "* S1= [1, 1, 1, 1, 0, 0, 0, 0]\n",
        "\n",
        "Pada dasarnya, feature vektor dibuat dengan mencari apakah kata dalam kosakata juga ditemukan dalam kalimat. Jika sebuah kata ditemukan dalam kosakata dan juga dalam kalimat, satu dimasukkan di tempat itu. Jika tidak, maka nol akan dimasukkan. Jadi untuk S1, empat kata pertama dalam kosakata hadir dalam kalimat S1, Kita memiliki empat yang di awal dan kemudian empat nol.\n",
        "\n",
        "Demikian pula, vektor fitur untuk S2 dan S3 adalah:\n",
        "\n",
        "* S2 = [0, 1, 1, 0, 1, 1, 0 , 0]\n",
        "\n",
        "* S3 = [0, 0, 0, 1, 0, 0, 1, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrnkY_ByDhsQ"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX-2wkvoDj98"
      },
      "source": [
        "Sekarang Kita tahu cara kerja dari bag of word. Sekarang kita akan melihat bagaimana TF-IDF terkait dengan bag of words.\n",
        "\n",
        "Dalam bag of words yang sederhana, setiap kata diberi arti penting yang sama. Gagasan di balik TF-IDF adalah bahwa kata-kata yang lebih sering muncul dalam satu dokumen dan lebih jarang di dokumen lain harus diberikan prioritas lebih tinggi karena mereka lebih berguna untuk klasifikasi.\n",
        "\n",
        "TF-IDF adalah produk dari dua istilah: TF dan IDF.\n",
        "\n",
        "**Term Frequency** adalah sama dengan berapa kali sebuah kata muncul dalam dokumen tertentu. Itu dihitung sebagai:\n",
        "\n",
        "`TF = (Frekuensi kata dalam dokumen) / (Total kata dalam dokumen)`\n",
        "\n",
        "**Inverse Document Frequency** adalah untuk kata tertentu sama dengan jumlah total dokumen, dibagi dengan jumlah dokumen yang mengandung kata tertentu. Log seluruh istilah dihitung untuk mengurangi dampak division. Itu dapat dihitung sebagai berikut:\n",
        "\n",
        "`IDF = Log ((Jumlah total dokumen) / (Jumlah dokumen yang mengandung kata))`\n",
        "\n",
        "Misalnya, dalam S1, TF untuk kata \"outside\" akan 1/4 = 0,25. Demikian pula, IDF untuk kata \"outside\" di S1 akan menjadi Log (3/2) = 0,176. Nilai dari TF-IDF akan menjadi 0,25 x 0,176 = 0,044.\n",
        "\n",
        "Ini adalah perhitungan yang rumit. Untungnya, Kita tidak harus melakukan semua perhitungan ini. Kelas `TfidfVectorizer` dari modul `sklearn.feature_extraction.text` dapat digunakan untuk membuat vektor fitur yang berisi nilai TF-IDF. Lihatlah skrip berikut:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8YwLIfgApr8"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidfconverter = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'),ngram_range=(1,3))\n",
        "X1 = tfidfconverter.fit_transform(processed_tweets).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMMsVmkwEHqW"
      },
      "source": [
        "Atribut **`max_features` menentukan jumlah kata yang paling banyak muncul** di mana Anda ingin membuat feature vektor. **Kata-kata yang jarang muncul tidak memainkan peran utama dalam klasifikasi.** Karenanya kami **hanya menyimpan 2000 kata yang paling sering muncul dalam dataset.** Nilai **`min_df` dari 5 menentukan bahwa kata tersebut harus muncul di setidaknya 5 dokumen**. Demikian pula, **nilai `max_df` sebesar 0,7 menetapkan bahwa kata tersebut tidak boleh muncul di lebih dari 70 persen dokumen.** Alasan di balik memilih 70 persen sebagai ambang batas adalah bahwa kata-kata yang muncul di lebih dari 70 persen dokumen terlalu umum dan kecil kemungkinannya untuk berperan dalam klasifikasi sentimen.\n",
        "\n",
        "Terakhir, untuk mengonversi dataset menjadi feature vektor TF-IDF yang sesuai, Kita perlu memanggil metode `fit_transform` pada kelas `TfidfVectorizer` dan meneruskannya dengan dataset yang telah kami proses sebelumnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz3iJizfGOZB"
      },
      "source": [
        "## Membagi Data ke Training dan Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYjm1EVIGTys"
      },
      "source": [
        "Sebelum membangun model analisis sentimen yang sebenarnya, bagi dataset ke training dan test set. Model akan melatih pada training set dan dievaluasi pada test set. Berikut script untuk membagi data menjadi training dan test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4Izr0lzEGgf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "272NNoFNGgCp"
      },
      "source": [
        "## 4. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o59LVui4V2OF"
      },
      "source": [
        "## Pelatihan dan Evaluasi dari Model Klasifikasi Teks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9YzCXQeGkVt"
      },
      "source": [
        "Kita telah membagi data ke dalam training dan test set, langkah selanjutnya adalah melatih model pada training set dan mengevaluasi kinerjanya pada test set. Sekarang, gunakan `RandomForestClassifier, Naive Bayes dan SVM` dari modul `sklearn` untuk melatih model. Kita dapat menggunakan classifier lainnya sesuai pilihan. Untuk melatih model, Kita perlu memanggil metode \"fit\" pada objek classifier dan meneruskannya ke training feature set dan training label set seperti yang ditunjukkan di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZwFF6ZXGfBj"
      },
      "source": [
        "import time\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "text_classifier_en = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "t0_en = time.time()\n",
        "text_classifier_en.fit(X_train, y_train)\n",
        "t1_en = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jizQpxeuJF_r"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "text_classifier_nb = GaussianNB()  \n",
        "t0_nb = time.time()\n",
        "text_classifier_nb.fit(X_train, y_train)\n",
        "t1_nb = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plO0fYm4zsyL"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "text_classifier_svm = SVC(kernel='linear')\n",
        "t0_svm = time.time()\n",
        "text_classifier_svm.fit(X_train, y_train)\n",
        "t1_svm = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH4yMdfVG0Tp"
      },
      "source": [
        "Untuk membuat prediksi pada test set, Kita harus melewati test set ke metode \"predict\" seperti yang ditunjukkan di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulqqo7MEGwsp"
      },
      "source": [
        "predictions_en = text_classifier_en.predict(X_test)\n",
        "t2_en = time.time()\n",
        "time_linear_train_en = t1_en-t0_en\n",
        "time_linear_predict_en = t2_en-t1_en\n",
        "\n",
        "predictions_nb = text_classifier_nb.predict(X_test)\n",
        "t2_nb = time.time()\n",
        "time_linear_train_nb = t1_nb-t0_nb\n",
        "time_linear_predict_nb = t2_nb-t1_nb\n",
        "\n",
        "predictions_svm = text_classifier_svm.predict(X_test)\n",
        "t2_svm = time.time()\n",
        "time_linear_train_svm = t1_svm-t0_svm\n",
        "time_linear_predict_svm = t2_svm-t1_svm\n",
        "\n",
        "# results\n",
        "print(\"EN Training time: %fs; Prediction time: %fs\" % (time_linear_train_en, time_linear_predict_en))\n",
        "print(\"NB Training time: %fs; Prediction time: %fs\" % (time_linear_train_nb, time_linear_predict_nb))\n",
        "print(\"SVM Training time: %fs; Prediction time: %fs\" % (time_linear_train_svm, time_linear_predict_svm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYpFcLPgHlBI"
      },
      "source": [
        "## 5. Evaluasi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ42kCO8WTDC"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt4e_45oG542"
      },
      "source": [
        "Terakhir, untuk mengevaluasi model klasifikasi yang digunakan, Kita dapat menggunakan *classification_report, confusion_matrix, accuracy_score, recall_score, precision_score,* dan *f1_score, roc_auc_score* sebagai performance metrics. Metrics ini dapat dihitung menggunakan class dari modul `sklearn.metrics` seperti yang ditunjukkan di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6U6-y21-qpF"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
        "\n",
        "print(\"Random Forest\")\n",
        "print('Accuracy  = ', round(accuracy_score(y_test, predictions_en)*100,2),'%')\n",
        "print('Recall    = ', round(recall_score(y_test, predictions_en)*100,2),'%')\n",
        "print('Precision = ', round(precision_score(y_test, predictions_en)*100,2),'%')\n",
        "print('F1-Score  = ', round(f1_score(y_test, predictions_en)*100,2),'%')\n",
        "print('ROC AUC   = ', roc_auc_score(y_test, predictions_en))\n",
        "print(\"\")\n",
        "print(\"Naive Bayes\")\n",
        "print('Accuracy  = ', round(accuracy_score(y_test, predictions_nb)*100,2),'%')\n",
        "print('Recall    = ', round(recall_score(y_test, predictions_nb)*100,2),'%')\n",
        "print('Precision = ', round(precision_score(y_test, predictions_nb)*100,2),'%')\n",
        "print('F1-Score  = ', round(f1_score(y_test, predictions_nb)*100,2),'%')\n",
        "print('ROC AUC   = ', roc_auc_score(y_test, predictions_nb))\n",
        "print(\"\")\n",
        "print(\"Support Vector Machine\")\n",
        "print('Accuracy  = ', round(accuracy_score(y_test, predictions_svm)*100,2),'%')\n",
        "print('Recall    = ', round(recall_score(y_test, predictions_svm)*100,2),'%')\n",
        "print('Precision = ', round(precision_score(y_test, predictions_svm)*100,2),'%')\n",
        "print('F1-Score  = ', round(f1_score(y_test, predictions_svm)*100,2),'%')\n",
        "print('ROC AUC   = ', roc_auc_score(y_test, predictions_svm))\n",
        "print(\"\")\n",
        "\n",
        "print(\"Random Forest\")\n",
        "print(confusion_matrix(y_test,predictions_en))  \n",
        "print(\"\")\n",
        "print(\"Naive Bayes\")\n",
        "print(confusion_matrix(y_test,predictions_nb))  \n",
        "print(\"\")\n",
        "print(\"Support Vector Machine\")\n",
        "print(confusion_matrix(y_test,predictions_svm))  \n",
        "print(\"\")\n",
        "\n",
        "print(\"Random Forest\")\n",
        "print(classification_report(y_test,predictions_en))  \n",
        "print(\"\")\n",
        "print(\"Naive Bayes\")\n",
        "print(classification_report(y_test,predictions_nb))  \n",
        "print(\"\")\n",
        "print(\"Support Vector Machine\")\n",
        "print(classification_report(y_test,predictions_svm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNNqcKS12rTq"
      },
      "source": [
        "## Testing Prediksi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC5bjvv6K2T8"
      },
      "source": [
        "# Random Forest\n",
        "# Input text\n",
        "review = \"comfort for passenger\"\n",
        "\n",
        "review_vector = tfidfconverter.transform([review]).toarray() # vectorizing\n",
        "pred_text = text_classifier_en.predict(review_vector)\n",
        "pred_text = le.inverse_transform(pred_text)\n",
        "print(pred_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ5d9sT9sJHM"
      },
      "source": [
        "# Naive Bayes\n",
        "# Input text\n",
        "review = \"comfort for passenger\"\n",
        "\n",
        "review_vector = tfidfconverter.transform([review]).toarray() # vectorizing\n",
        "pred_text = text_classifier_nb.predict(review_vector)\n",
        "pred_text = le.inverse_transform(pred_text)\n",
        "print(pred_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq7kfKf2sToy"
      },
      "source": [
        "# SVM\n",
        "# Input text\n",
        "review = \"there is delays\"\n",
        "\n",
        "review_vector = tfidfconverter.transform([review]).toarray() # vectorizing\n",
        "pred_text = text_classifier_svm.predict(review_vector)\n",
        "pred_text = le.inverse_transform(pred_text)\n",
        "print(pred_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DraYPLXOHrMM"
      },
      "source": [
        "## Kesimpulan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiZrDPnbHv_w"
      },
      "source": [
        "Sentiment Analysis adalah salah satu tugas utama dalam proses natural language. Untuk menerapkan teknik statistik untuk Sentiment Analysis perlu mengubah teks menjadi angka. Pada pembahasan ini, melihat bagaimana pendekatan TF-IDF dapat digunakan untuk membuat vektor fitur numerik dari teks. Model Sentiment Analysis tertinggi diatas mencapai akurasi sekitar 91.47 % untuk prediksi sentimen menggunakan SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ_dH8BS53Ew"
      },
      "source": [
        "References:\n",
        "\n",
        "[Dataset](https://www.kaggle.com/crowdflower/twitter-airline-sentiment?select=Tweets.csv)\n",
        "\n",
        "[Jurnal](http://journal.uad.ac.id/index.php/TELKOMNIKA/article/view/14179)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ATwec_wrRCG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}